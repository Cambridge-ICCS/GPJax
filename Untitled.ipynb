{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from flax import struct\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap, grad, jit\n",
    "from typing import Callable\n",
    "\n",
    "key = jr.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predict function from a set of parameters\n",
    "def make_predict(W,b):\n",
    "    def predict(x):\n",
    "        return jnp.dot(W,x)+b\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss from the data points set\n",
    "def make_mse(x_batched,y_batched):\n",
    "    def mse(W,b):\n",
    "        # Define the squared loss for a single pair (x,y)\n",
    "        def squared_error(x,y):\n",
    "            y_pred = make_predict(W,b)(x)\n",
    "            return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
    "        # We vectorize the previous to compute the average of the loss on all samples.\n",
    "        return jnp.mean(vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "    return jit(mse) # And finally we jit the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set problem dimensions\n",
    "nsamples = 20\n",
    "xdim = 10\n",
    "ydim = 5\n",
    "\n",
    "# Generate random ground truth W and b\n",
    "k1, k2 = jr.split(key)\n",
    "W = jr.normal(k1, (ydim, xdim))\n",
    "b = jr.normal(k2, (ydim,))\n",
    "true_predict = make_predict(W,b)\n",
    "\n",
    "# Generate samples with additional noise\n",
    "ksample, knoise = jr.split(k1)\n",
    "x_samples = jr.normal(ksample, (nsamples, xdim))\n",
    "y_samples = vmap(true_predict)(x_samples) + 0.1*jr.normal(knoise,(nsamples, ydim))\n",
    "\n",
    "# Generate MSE for our samples\n",
    "mse = make_mse(x_samples,y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.026416078\n",
      "Loss step 0:  11.808268\n",
      "Loss step 20:  0.07759722\n",
      "Loss step 40:  0.02324226\n",
      "Loss step 60:  0.0143046575\n",
      "Loss step 80:  0.012400283\n",
      "Loss step 100:  0.01197335\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimated W and b with zeros.\n",
    "What = jnp.zeros_like(W)\n",
    "bhat = jnp.zeros_like(b)\n",
    "\n",
    "alpha = 0.3 # Gradient step size\n",
    "print('Loss for \"true\" W,b: ', mse(W,b))\n",
    "for i in range(101):\n",
    "    # We perform one gradient update\n",
    "    What, bhat = What - alpha*grad(mse,0)(What,bhat), bhat - alpha*grad(mse,1)(What,bhat)\n",
    "    if (i%20==0):\n",
    "        print(\"Loss step {}: \".format(i), mse(What,bhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flax setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create one dense layer instance (taking 'features' parameter as input)\n",
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({'params': {'bias': (5,), 'kernel': (10, 5)}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, subkey = jr.split(key)\n",
    "x = jr.normal(key, (10, ))\n",
    "params = model.init(key, x)\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of x controls the shape of the model's parameters. `model.init` controls the initialisation of model parameters and returns the params as a frozen dictionary; an immutable structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 1.3209158,  0.1376045,  0.6276722,  1.2021543, -1.1306524],            dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model at some data points x for a given set of params\n",
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set problem dimensions\n",
    "nsamples = 20\n",
    "xdim = 10\n",
    "ydim = 5\n",
    "\n",
    "# Generate random ground truth W and b\n",
    "key = jr.PRNGKey(0)\n",
    "k1, k2 = jr.split(key)\n",
    "W = jr.normal(k1, (xdim, ydim))\n",
    "b = jr.normal(k2, (ydim,))\n",
    "true_params = freeze({'params': {'bias': b, 'kernel': W}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Generate samples with additional noise\n",
    "ksample, knoise = jr.split(k1)\n",
    "x_samples = jr.normal(ksample, (nsamples, xdim))\n",
    "y_samples = jnp.dot(x,W) + b\n",
    "y_samples += 0.1*jr.normal(knoise,(nsamples, ydim)) # Adding noise\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mse_func(x_batched, y_batched):\n",
    "    def mse(params):\n",
    "    # Define the squared loss for a single pair (x,y)\n",
    "        def squared_error(x, y):\n",
    "            pred = model.apply(params, x)\n",
    "            return jnp.inner(y-pred, y-pred)/2.0\n",
    "        # We vectorize the previous to compute the average of the loss on all samples.\n",
    "        return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "    return jax.jit(mse) # And finally we jit the result.\n",
    "\n",
    "# Get the sampled loss\n",
    "loss = make_mse_func(x_samples, y_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  29.405216\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.3 # Gradient step size\n",
    "print('Loss for \"true\" W,b: ', loss(true_params))\n",
    "grad_fn = jax.value_and_grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  24.20204\n",
      "Loss step 10:  0.35897788\n",
      "Loss step 20:  0.06296228\n",
      "Loss step 30:  0.023336558\n",
      "Loss step 40:  0.01489224\n",
      "Loss step 50:  0.0125975255\n",
      "Loss step 60:  0.011899963\n",
      "Loss step 70:  0.011677139\n",
      "Loss step 80:  0.0116043445\n",
      "Loss step 90:  0.011580311\n",
      "Loss step 100:  0.011572338\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    loss_val, grad = grad_fn(params)\n",
    "    params = jax.tree_multimap(lambda old, grad: old - alpha * grad, params, grad)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an optimiser\n",
    "optimizer_def = optim.GradientDescent(learning_rate=alpha) # Choose the method\n",
    "# Create the wrapping optimizer with initial parameters\n",
    "optimizer = optimizer_def.create(params) \n",
    "loss_grad_fn = jax.value_and_grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011568356\n",
      "Loss step 10:  0.011568359\n",
      "Loss step 20:  0.011568356\n",
      "Loss step 30:  0.01156836\n",
      "Loss step 40:  0.011568361\n",
      "Loss step 50:  0.011568356\n",
      "Loss step 60:  0.011568361\n",
      "Loss step 70:  0.01156836\n",
      "Loss step 80:  0.011568353\n",
      "Loss step 90:  0.011568354\n",
      "Loss step 100:  0.011568355\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    loss_val, grad = loss_grad_fn(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grad) # Return the updated optimizer with parameters.\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-ee8783676bbf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-ee8783676bbf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class GaussianProcess\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class GaussianProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, feat in enumerate(self.features):\n",
    "            x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "            if i != len(self.features) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.features + other.features\n",
    "\n",
    "m1 = SimpleMLP(features=5)\n",
    "m2 = SimpleMLP(features=10)\n",
    "m1+m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPBlocks",
   "language": "python",
   "name": "gpblocks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
