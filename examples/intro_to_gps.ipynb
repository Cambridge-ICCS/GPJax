{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Gaussian Processes\n",
    "\n",
    "In this notebook we provide the material required to understand Gaussian processes (GPs). No prior knowledge of Bayesian inference or GPs is assumed, and this notebook is self-contained. At a high level, we begin mathbf{y} introducing Bayes' theorem and its implications within probabilistic modelling. We then proceed to introduce the Gaussian random variable along with its multivariate form. We conclude mathbf{y} showing how this notion can be extended to GPs.\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "\n",
    "A probabilistic modelling task is comprised of an observed dataset $\\mathbf{y}$ for\n",
    "which we construct a model. The parameters $\\theta$ of our model are unknown,\n",
    "and our goal is to conduct inference to determine their range of likely values.\n",
    "To achieve this, we apply Bayes' theorem\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\theta\\,|\\, \\mathbf{y}) = \\mathbf{f}rac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{p(\\mathbf{y})} = \\mathbf{f}rac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{\\int_{\\theta}p(\\mathbf{y}, \\theta)\\mathrm{d}\\theta}\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where $p(\\mathbf{y}\\,|\\,\\theta)$ denotes the _likelihood_, or model, and\n",
    "quantifies how likely the observed dataset $\\mathbf{y}$ is, given the parameter\n",
    "estimate $\\theta$. The _prior_ distribution $p(\\theta)$ reflects our\n",
    "initial beliefs about the value of $\\theta$ before observing data, whilst the\n",
    "_posterior_ $p(\\theta\\,|\\, \\mathbf{y})$ gives an updated estimate of the\n",
    "parameters' value, after observing $\\mathbf{y}$. The _marginal likelihood_, or\n",
    "Bayesian model evidence, $p(\\mathbf{y})$ is the probability of the observed data under\n",
    "all possible hypotheses that our prior model can generate. Within Bayesian model selection, this property\n",
    "makes the marginal log-likelihood an indispensable tool. Selecting models under\n",
    "this criterion places a higher emphasis on models that can generalise better to\n",
    "new data points.\n",
    "\n",
    "When the posterior distribution belongs to the same family of probability\n",
    "distributions as the prior, we describe the prior and the likelihood as\n",
    "_conjugate_ to each other. Such a scenario is convenient in Bayesian\n",
    "inference as it allows us to derive closed-form expressions for the posterior\n",
    "distribution. When the likelihood function is a member of the exponential\n",
    "family, then there exists a conjugate prior. However, the conjugate prior may\n",
    "not have a form that precisely reflects the practitioner's belief surrounding\n",
    "the parameter. For this reason, conjugate models seldom appear; one exception to\n",
    "this is GP regression that we present fully in our [Regression notebook](https://gpjax.readthedocs.io/en/latest/examples/regression.html).\n",
    "\n",
    "For models that do not contain a conjugate prior, the marginal\n",
    "log-likelihood must be calcualted to normalise the posterior distribution and ensure it integrates\n",
    "to 1. For models with a single, 1-dimensional parameter, it may be possible to\n",
    "compute this integral analytically or through a quadrature scheme, such as\n",
    "Gauss-Hermite. However, in machine\n",
    "learning, the dimensionality of $\\theta$ is often large and the corresponding\n",
    "integral required to compute $p(\\mathbf{y})$ quickly becomes intractable as the\n",
    "dimension grows. Techniques such as Markov Chain Monte Carlo and variational inference allow us to approximate integrals such as the one seen in $p(\\mathbf{y})$.\n",
    "\n",
    "Once a posterior distribution has been obtained, we can make predictions at new\n",
    "points $\\mathbf{y}^{\\star}$ through the \\textit{posterior predictive distribution}. This is\n",
    "achieved by integrating out the parameter set $\\theta$ from our posterior\n",
    "distribution through\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{y}^{\\star}\\,|\\, \\mathbf{y}) & = \\int p(\\mathbf{y}^{\\star},\\theta \\,|\\,  \\mathbf{y} )\\mathrm{d}\\theta \\\\\n",
    "    & = \\int p(\\mathbf{y}^{\\star} \\,|\\, \\theta, \\mathbf{y} ) p(\\theta\\,|\\, \\mathbf{y})\\mathrm{d}\\theta\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "As with the marginal log-likelihood, evaluating this quantity requires computing\n",
    "an integral which may not be tractable, particularly when $\\theta$ is\n",
    "high-dimensional.\n",
    "\n",
    "It is difficult to communicate statistics directly through a posterior\n",
    "distribution, so we often compute and report moments of the posterior\n",
    "distribution. Most commonly, we report the first moment and the centred second\n",
    "moment\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "    \\mu  = \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]  & = \\int \\theta\n",
    "    p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta &\\\\\n",
    "    \\sigma^2  = \\mathbb{V}[\\theta\\,|\\,\\mathbf{y}] & = \\int \\left(\\theta -\n",
    "    \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]\\right)^2p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta&\\,.\n",
    "\\end{alignat}\n",
    "$$\n",
    "Through this pair of statistics, we can communicate our beliefs about the most\n",
    "likely value of $\\theta$ i.e., $\\mu$, and the uncertainty $\\sigma$ around the\n",
    "expected value. However, as with the marginal log-likelihood and predictive\n",
    "posterior distribution, computing these statistics again requires a potentially\n",
    "intractable integral.\n",
    "\n",
    "## Gaussian random variables\n",
    "\n",
    "We begin our review with the simplest case; a univariate Gaussian random\n",
    "variable. For a random variable $y$, let $\\mu\\in\\mathbb{R}$ be a mean scalar and\n",
    "$\\sigma^2\\in\\mathbb{R}_{>0}$ a variance scalar. If $y$ is a Gaussian random\n",
    "variable, then the density of $y$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{N}(y\\,|\\, \\mu, \\sigma^2) = \\mathbf{f}rac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\mathbf{f}rac{(y-\\mu)^2}{2\\sigma^{2}}\\right)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "We can plot three different parameterisations of this density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import distrax as dx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ud1 = dx.Normal(0.0, 1.0)\n",
    "ud2 = dx.Normal(-1.0, 0.5)\n",
    "ud3 = dx.Normal(0.25, 1.5)\n",
    "\n",
    "xs = jnp.linspace(-5.0, 5.0, 500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "for d in [ud1, ud2, ud3]:\n",
    "    ax.plot(xs, d.prob(xs), label=f'$\\mathcal{{N}}({{{float(d.mean())}}},\\  {{{float(d.stddev())}}}^2)$' )\n",
    "    ax.fill_between(xs, jnp.zeros_like(xs), d.prob(xs), alpha=0.2)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian random variable is uniquely defined in distribution by its mean $\\mu$\n",
    "and variance $\\sigma^2$ and we therefore write $y\\sim\\mathcal{N}(\\mu, \\sigma^2)$ when\n",
    "describing a Gaussian random variable. Further, we can compute these two\n",
    "quantities by\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[y] = \\mu\\,, \\quad \\quad \\mathbb{E}\\left[(y-\\mu)^2\\right] =\\sigma^2\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Extending this concept to vector-valued random variables reveals the\n",
    "multivariate Gaussian random variables which brings us closer to the full\n",
    "definition of a GP.\n",
    "\n",
    "Let $\\mathbf{y}$ be a $D$-dimensional random variable, $\\boldsymbol{\\mu}$ be a $D$-dimensional\n",
    "mean vector and $\\boldsymbol{\\Sigma}$ be a $D\\times D$ covariance matrix. If $\\mathbf{y}$ is a\n",
    "Gaussian random variable, then the density of $\\mathbf{y}$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{N}(\\mathbf{y}\\,|\\, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\mathbf{f}rac{1}{\\sqrt{2\\pi}^{D/2} \\lvert\\boldsymbol{\\Sigma}\\rvert^{1/2}} \\exp\\left(-\\mathbf{f}rac{1}{2} \\left(\\mathbf{y} - \\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{y}-\\boldsymbol{\\mu}\\right) \\right) \\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Three example parameterisations of this can be visualised below where $\\rho$ determines the correlation of the multivariate Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import confidence_ellipse\n",
    "import jax.random as jr \n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "d1 = dx.MultivariateNormalDiag(jnp.zeros(2), scale_diag=jnp.array([1.0, 1.0]))\n",
    "d2 = dx.MultivariateNormalTri(jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]])))\n",
    "d3 = dx.MultivariateNormalTri(jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]])))\n",
    "\n",
    "dists = [d1, d2, d3]\n",
    "\n",
    "xvals = jnp.linspace(-5.0, 5.0, 500)\n",
    "yvals = jnp.linspace(-5.0, 5.0, 500)\n",
    "\n",
    "xx, yy = jnp.meshgrid(xvals, yvals)\n",
    "\n",
    "pos = jnp.empty(xx.shape + (2,))\n",
    "pos.at[:, :, 0].set(xx)\n",
    "pos.at[:, :, 1].set(yy)\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\n",
    "titles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\n",
    "\n",
    "for a, t, d in zip([ax0, ax1, ax2], titles, dists):\n",
    "    d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(xx.shape)\n",
    "    cntf = a.contourf(xx, yy, d_prob, levels=20, antialiased=True, cmap = 'Reds')\n",
    "    for c in cntf.collections:\n",
    "        c.set_edgecolor(\"face\")\n",
    "    a.set_xlim(-2.75, 2.75)\n",
    "    a.set_ylim(-2.75, 2.75)\n",
    "    samples = d.sample(seed = key, sample_shape=(5000,))\n",
    "    xsample, ysample = samples[:, 0], samples[:, 1]\n",
    "    confidence_ellipse(\n",
    "        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n",
    "    )\n",
    "    confidence_ellipse(xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\")\n",
    "    a.plot(0, 0, \"x\", color=\"tab:blue\", markersize=8, mew=2)\n",
    "    a.set(xlabel=\"x\", ylabel=\"y\", title=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the intuition given for the moments of a univariate Gaussian random variables, we can obtain the mean and covariance by\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[\\mathbf{y}] = \\boldsymbol{\\mu}\\,, \\quad \\operatorname{Cov}(\\mathbf{y}) & =\\mathbb{E}\\left[(\\mathbf{y} - \\boldsymbol{\\mu})(\\mathbf{y} - \\boldsymbol{\\mu})^{\\top}\\right] \\\\\n",
    "    & = \\mathbb{E}[\\mathbf{y}^2] - \\mathbb{E}[\\mathbf{y}]^2 \\\\\n",
    "    & = \\boldsymbol{\\Sigma}\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "The covariance matrix is a symmetric positive definite matrix that generalises\n",
    "the notion of variance to multiple dimensions. The matrix's diagonal entries\n",
    "contain the variance of each element, whilst the off-diagonal entries quantify\n",
    "the degree to which the respective pair of random variables are linearly\n",
    "related; this quantity is called the _covariance_.\n",
    "\n",
    "Assuming a Gaussian likelihood function in a Bayesian model is attractive as the\n",
    "mean and variance parameters are highly interpretable. This makes prior\n",
    "elicitation straightforward as the parameters' value can be intuitively\n",
    "contextualised within the scope of the problem at hand. Further, in models where\n",
    "the posterior distribution is Gaussian, we again use the distribution's mean and\n",
    "variance to describe our prediction and corresponding uncertainty around a given\n",
    "event occurring.\n",
    "\n",
    "Not only are Gaussian random variables highly interpretable, but linear\n",
    "operations involving them lead to analytical solutions. An\n",
    "example of this that will be useful in the sequel is the marginalisation and\n",
    "conditioning property of sets of Gaussian random variables. We will present\n",
    "these two results now for a pair of Gaussian random variables, but it should be\n",
    "stressed that these results hold for any finite set of Gaussian random\n",
    "variables.\n",
    "\n",
    "For a pair of random variables $\\mathbf{x}$ and $\\mathbf{y}$ defined on the same support, the\n",
    "distribution over them both is known as the _joint distribution_. The\n",
    "joint distribution $p(\\mathbf{x}, \\mathbf{y})$ quantifies the probability of two events, one\n",
    "from $p(\\mathbf{x})$ and another from $p(\\mathbf{y})$, occurring at the same time. We visualise this idea below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "\n",
    "n = 1000\n",
    "x = dx.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\n",
    "key, subkey = jr.split(key)\n",
    "y = dx.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\n",
    "key, subkey = jr.split(subkey)\n",
    "xfull =  dx.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n*10,))\n",
    "key, subkey = jr.split(subkey)\n",
    "yfull = dx.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n*10,))\n",
    "key, subkey = jr.split(subkey)\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\n",
    "\n",
    "g = sns.jointplot(\n",
    "    data=df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"idx\",\n",
    "    marker=\".\",\n",
    "    space=0.0,\n",
    "    xlim=(-4.0, 4.0),\n",
    "    ylim=(-4.0, 4.0),\n",
    "    height=4,\n",
    "    marginal_ticks=False,\n",
    "    legend=False,\n",
    "    marginal_kws={\n",
    "        \"fill\": True,\n",
    "        \"linewidth\": 1,\n",
    "        \"color\": \"#A60628\",\n",
    "        \"alpha\": 0.3,\n",
    "        \"bw_adjust\": 2,\n",
    "    },\n",
    "    joint_kws={\"color\": \"#A60628\", \"size\": 3.5, \"alpha\": 0.4},\n",
    ")\n",
    "g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\n",
    "g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\n",
    "g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\n",
    "confidence_ellipse(\n",
    "    xfull, yfull, g.ax_joint, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", linewidth=0.5\n",
    ")\n",
    "confidence_ellipse(\n",
    "    xfull, yfull, g.ax_joint, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\", linewidth=0.5\n",
    ")\n",
    "confidence_ellipse(\n",
    "    xfull, yfull, g.ax_joint, edgecolor=\"#3f3f3f\", n_std=3.0, linestyle=\"--\", linewidth=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formmally, we can define this by letting $p(\\mathbf{x}, \\mathbf{y})$ be the joint probability distribution defined over $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}}, \\boldsymbol{\\Sigma}_{\\mathbf{xx}})$ and $\\mathbf{y}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}})$. We define the joint distribution as\n",
    "$$\n",
    "\\begin{align}\n",
    "    p\\left(\\begin{bmatrix}\n",
    "        \\mathbf{x} \\\\ \\mathbf{y}\n",
    "    \\end{bmatrix}\\right) = \\mathcal{N}\\left(\\begin{bmatrix}\n",
    "        \\boldsymbol{\\mu}_{\\mathbf{x}} \\\\ \\boldsymbol{\\mu}_{\\mathbf{y}}\n",
    "    \\end{bmatrix}, \\begin{bmatrix}\n",
    "        \\boldsymbol{\\Sigma}_{\\mathbf{xx}}, \\boldsymbol{\\Sigma}_{\\mathbf{xy}} \\\\\n",
    "        \\boldsymbol{\\Sigma}_{\\mathbf{yx}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}}\n",
    "    \\end{bmatrix} \\right)\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\boldsymbol{\\Sigma}_{\\mathbf{x}\\mathbf{y}}$ is the cross-covariance matrix of $\\mathbf{x}$ and $\\mathbf{y}$.\n",
    "\n",
    "When presented with a joint distribution, two tasks that we may wish to perform\n",
    "are \\textit{marginalisation} and \\textit{conditioning}. For a joint distribution\n",
    "$p(\\mathbf{x}, \\mathbf{y})$ where we are interested only in $p(\\mathbf{x})$, we must integrate over all\n",
    "possible values of $\\mathbf{y}$ to obtain $p(\\mathbf{x})$. This process is marginalisation.\n",
    "Conditioning allows us to evaluate the probability of one random variable, given\n",
    "that the other random variable is fixed. For a joint Gaussian distribution,\n",
    "marginalisation and conditioning have analytical expressions where the resulting\n",
    "distribution is also a Gaussian random variable.\n",
    "\n",
    "For a joint Gaussian random variable, the marginalisation of $\\mathbf{x}$ or $\\mathbf{y}$ is given by\n",
    "$$\n",
    "\\begin{alignat}{3}\n",
    "    & \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{y} && = p(\\mathbf{x})\n",
    "    && = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}},\n",
    "    \\boldsymbol{\\Sigma}_{\\mathbf{xx}}) \\\\\n",
    "    & \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{x} && = p(\\mathbf{y})\n",
    "    && = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}},\n",
    "    \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\,.\n",
    "\\end{alignat}\n",
    "$$\n",
    "The conditional distributions are given by\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{x}\\,|\\, \\mathbf{y}) & = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{x}} + \\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\boldsymbol{\\Sigma}_{\\mathbf{yy}}^{-1}(\\mathbf{y}-\\boldsymbol{\\mu}_{\\mathbf{y}}), \\boldsymbol{\\Sigma}_{\\mathbf{xx}}-\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\boldsymbol{\\Sigma}_{\\mathbf{yy}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\right)\\\\\n",
    "    p(\\mathbf{y}\\,|\\, \\mathbf{x}) & = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{y}} + \\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{\\mathbf{x}}), \\boldsymbol{\\Sigma}_{\\mathbf{yy}}-\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\right)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Within this section, we have introduced the idea of multivariate Gaussian random\n",
    "variables and presented some key results concerning their properties. In the\n",
    "following section, we will lift our presentation of Gaussian random variables to\n",
    "GPs.\n",
    "\n",
    "## Gaussian processes\n",
    "\n",
    "When transitioning\n",
    "from Gaussian random variables to GP there is a shift in thought\n",
    "required to parse the forthcoming material. Firstly, to be consistent with\n",
    "the general literature, we hereon use $\\mathbf{X}$ to denote an observed vector of data\n",
    "points, not a random variable as has been true up until now. To distinguish\n",
    "between matrices and vectors, we use bold upper case characters e.g., $\\mathbf{X}$ for\n",
    "matrices, and bold lower case characters for vectors e.g., $\\mathbf{X}$.\n",
    "\n",
    "We are interested in modelling supervised learning problems, where we have $n$\n",
    "observations $\\mathbf{y}=\\{y_1, y_2,\\ldots ,y_n\\}\\subset\\mathcal{Y}$ at corresponding inputs\n",
    "$\\mathbf{X}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\subset\\mathcal{X}$. We aim to capture the relationship\n",
    "between $\\mathbf{X}$ and $\\mathbf{y}$ using a model $f$ with which we may make predictions at\n",
    "an unseen set of test points $\\mathbf{X}^{\\star}\\subset\\mathcal{X}$. We formalise this by\n",
    "$$\n",
    "\\begin{align}\n",
    "    y = f(\\mathbf{X}) + \\varepsilon\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\varepsilon$ is an observational noise term. We collectively refer to $(\\mathbf{X}, \\mathbf{y})$ as\n",
    "the \\textit{training data} and $\\mathbf{X}^{\\star}$ as the set of \\textit{test points}.\n",
    "This process is visualised below\n",
    "\n",
    "![](figs/generating_process.png)\n",
    "\n",
    "As we shall\n",
    "go on to see, GPs offer an appealing workflow for scenarios such as this,\n",
    "all under a Bayesian framework.\n",
    "\n",
    "We write a GP $f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))$ with mean\n",
    "function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and $\\boldsymbol{\\theta}$-parameterised kernel\n",
    "$k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$. When evaluating the GP on a\n",
    "finite set of points $\\mathbf{X}\\subset\\mathcal{X}$, $k$ gives rise to the Gram matrix $\\mathbf{K}_{ff}$\n",
    "such that the $(i, j)^{\\text{th}}$ entry of the matrix is given by $[\\mathbf{K}_{ff}]_{i,\n",
    "j} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. As is conventional within the literature, we centre our\n",
    "training data and assume $\\mu(\\mathbf{X}):= 0$ for all $\\mathbf{X}\\in\\mathbf{X}$. We further drop dependency\n",
    "on $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ for notational convenience in the remainder of this\n",
    "article.\n",
    "\n",
    "We define a joint GP prior over the latent function\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{f}, \\mathbf{f}^{\\star}) = \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix}\n",
    "        \\mathbf{K}_{ff} & \\mathbf{K}_{fx} \\\\\n",
    "        \\mathbf{K}_{xf} & \\mathbf{K}_{xx}\n",
    "    \\end{bmatrix}\\right)\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})$. Conditional on the GP's latent\n",
    "function $f$, we assume a factorising likelihood generates our observations\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{y}\\,|\\,\\mathbf{f}) = \\prod_{i=1}^n p(y_i\\,|\\, f_i)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Strictly speaking, the likelihood function is $p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))$ where $\\phi$\n",
    "is the likelihood function's associated link function. Example link functions\n",
    "include the probit or logistic functions for a Bernoulli likelihood and the\n",
    "identity function for a Gaussian likelihood. We eschew this notation for now as\n",
    "this section primarily considers Gaussian likelihood functions where the role of $\\phi$ is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in [classification](https://gpjax.readthedocs.io/en/latest/examples/classification.html).\n",
    "\n",
    "Applying Bayes' theorem yields the joint posterior distribution over the latent\n",
    "function\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) = \\mathbf{f}rac{p(\\mathbf{y}\\,|\\,\\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})}{p(\\mathbf{y})}\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The choice of kernel function that we use to parameterise our GP is an\n",
    "important modelling decision as the choice of kernel dictates properties such as\n",
    "differentiability, variance and characteristic lengthscale of the functions that\n",
    "are admissible under the GP prior. A kernel is\n",
    "a positive-definite function with parameters $\\boldsymbol{\\theta}$ that maps pairs of inputs\n",
    "$\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}$ onto the real line. We dedicate the entirity of the [Kernel Guide notebook](https://gpjax.readthedocs.io/en/latest/examples/kernels.html) to exploring the different GPs each kernel can yield.\n",
    "\n",
    "## Gaussian process regression\n",
    "\n",
    "When the likelihood function is a Gaussian distribution\n",
    "$p(y_i\\,|\\, f_i) = \\mathcal{N}(y_i\\,|\\, f_i, \\sigma_n^2)$, marginalising $\\mathbf{f}$ from the\n",
    "joint posterior to obtain the posterior predictive distribution is exact\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) & = \\int p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\, \\mathbf{y})\\mathrm{d}\\mathbf{f} \\\\\n",
    "    & = \\mathcal{N}(\\mathbf{f}^{\\star}\\,|\\,\\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}}, \\Sigma_{\\,|\\,\\mathbf{y}})\\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}} & = \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n\\mathbf{I}_n \\right)^{-1}\\mathbf{y}\\\\\n",
    "    \\Sigma_{\\,|\\,\\mathbf{y}} & = \\mathbf{K}_{\\star\\star} - \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n\\right)^{-1}\\mathbf{K}_{fx} \\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Further, the log of the  marginal likelihood of the GP can\n",
    "be analytically expressed as\n",
    "$$\n",
    "\\begin{align}\n",
    "        \\log p(\\mathbf{y}) & = \\log\\int p(\\mathbf{y}\\,|\\, \\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})\\mathrm{d}\\mathbf{f}^{\\star}  \\\\\n",
    "        & = 0.5\\left(-\\underbrace{\\mathbf{y}^{\\top}\\left(\\mathbf{K}_{ff} - \\sigma_n^2\\mathbf{I}_n \\right)^{-1}\\mathbf{y}}_{\\text{Data fit}} -\\underbrace{\\log\\lvert \\mathbf{K}_{ff} + \\sigma^2_n\\rvert}_{\\text{Complexity}} -\\underbrace{n\\log 2\\pi}_{\\text{Constant}} \\right)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Model selection can be performed for a GP through gradient-based\n",
    "optimisation of $\\log p(\\mathbf{y})$ with respect to the kernel's parameters $\\boldsymbol{\\theta}$\n",
    "and the observational noise $\\sigma^2_n$. Collectively, we call these terms the\n",
    "model hyperparameters $\\boldsymbol{\\xi} = \\{\\boldsymbol{\\theta}, \\sigma_n^2\\}$ from which the maximum\n",
    "likelihood estimate is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{\\xi}^{\\star} = \\argmax_{\\boldsymbol{\\xi} \\in \\Xi} \\log p(\\mathbf{y})\\,.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Observing the individual terms in the marginal log-likelihood can help\n",
    "understand exactly why optimising the marginal log-likelihood gives reasonable\n",
    "solutions. The _data fit_ term is the only component of\n",
    "the marginal log-likelihood that includes the observed response $\\mathbf{y}$ and\n",
    "will therefore encourage solutions that model the data well. Conversely, the\n",
    "_complexity_ term contains a determinant operator and therefore measures\n",
    "the _volume_ of the function space covered by the GP. Whilst a more\n",
    "complex function has a better chance of modelling the observed data well, this\n",
    "is only true to a point and functions that are overly complex will overfit the\n",
    "data. Optimising with respect to the marginal log-likelihood balances\n",
    "these two objectives when identifying the optimal solution, as visualised below.\n",
    "\n",
    "![](figs/decomposed_mll.png)\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Within this notebook we have built up the concept of a GP, starting from Bayes' theorem and the definition of a Gaussian random variable. Using the ideas presented in this notebook, the user should be in a position to dive into our [Regression notebook](https://gpjax.readthedocs.io/en/latest/examples/regression.html) and start getting their hands on some code. For those looking to learn more about the underling theory of GPs, an excellent starting point is the [Gaussian Processes for Machine Learning](http://gaussianprocess.org/gpml/) textbook. Alternatively, the [thesis of Alexander Terenin](https://arxiv.org/abs/2202.10613) provides a rigorous exposition of GPs that served as the inspiration for this notebook.\n",
    "\n",
    "## System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('gpjax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "920091140e6b97de16b405af485d142952a229f5dad61a888f46227f5acb94cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
