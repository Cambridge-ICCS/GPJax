# ---
# jupyter:
#   jupytext:
#     custom_cell_magics: kql
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.2
#   kernelspec:
#     display_name: Python 3.9.7 ('gpjax')
#     language: python
#     name: python3
# ---

# %% [markdown] pycharm={"name": "#%% md\n"}
# # Regression
# %%
from pprint import PrettyPrinter
from weakref import WeakKeyDictionary

import jax
import jax.numpy as jnp
import jax.random as jr
import matplotlib.pyplot as plt
from jax.example_libraries import optimizers

import gpjax as gpx

pp = PrettyPrinter(indent=4)
key = jr.PRNGKey(123)

# %% [markdown]
# ## Dataset
#
# With the necessary libraries and modules imported, we simulate a dataset comprising 50 datapoints with inputs $\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy observations
#
# $$\boldsymbol{y} \sim \mathcal{N} \left(\sin(4\boldsymbol{x}) + \cos(2 \boldsymbol{x}), \boldsymbol{I} * 0.3^2 \right).$$
#
# We store this data $\mathcal{D} = (\boldsymbol{x}, \boldsymbol{y})$ as a GPJax `Dataset` and create some test inputs and labels.

# %%
N = 50
noise = 0.3

x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(N,)).sort().reshape(-1, 1)
f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)
signal = f(x)
y = signal + jr.normal(key, shape=signal.shape) * noise

D = gpx.Dataset(X=x, y=y)

xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)
ytest = f(xtest)
# %% [markdown]
# To better understand the data that we have simulated, we plot both the underlying latent function and the observed data which is subject to Gaussian noise.

# %%
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(xtest, ytest, label="Latent function")
ax.plot(x, y, "o", label="Observations")
ax.legend(loc="best")

# %% [markdown]
# Our aim in this tutorial will be to recreate the latent function from our noisy observations $\mathcal{D}$ via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.


# %% [markdown]
# ## Defining the prior
#
# A zero-mean Gaussian process places a prior distribution over real-valued functions $f(\cdot)$, such that $f(\boldsymbol{x}) \sim \mathcal{N}(0, \boldsymbol{K}_{\boldsymbol{x}\boldsymbol{x}})$ where $\boldsymbol{K}_{\boldsymbol{x}\boldsymbol{x}}$ is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function $k(\cdot, \cdot')$ such that $[\boldsymbol{K}_{\boldsymbol{x}\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$. The choice of kernel function is an important choice as, among other things, it governs the smoothness of the outputs that our Gaussian process can generate. For now, we'll use a squared exponential kernel which takes the form
#
# $$k(\boldsymbol{x}, \boldsymbol{x}') = \sigma^2 \exp\left(-\frac{\lVert \boldsymbol{x}-\boldsymbol{x}' \rVert_2^2}{2 \ell^2}\right).$$
#
# As we can see, the squared exponential kernel has two parameters; a lengthscale $\ell$ that controls horizontal displacement, and a variance term $\sigma$ that controls vertical displacement.

# %%
kernel = gpx.RBF()
prior = gpx.Prior(kernel=kernel)

# %% [markdown]
# ### Visualising the prior
#
# Each Gaussian process in GPJax can be represented by a [Distrax](https://github.com/deepmind/distrax) multivariate Gaussian distribution. Such functionality enables trivial sampling from the GP, mean and covariance evaluation of the GP.

# %%
params, trainable, constrainers, unconstrainers = gpx.initialise(prior)
prior_dist = prior(params)(xtest)

prior_mean = prior_dist.mean()
prior_std = jnp.sqrt(prior_dist.covariance().diagonal())
samples = prior_dist.sample(seed=key,sample_shape=20).T

plt.plot(xtest, samples, color='tab:blue', alpha=0.5)
plt.plot(xtest, prior_mean, color='tab:orange')
plt.fill_between(xtest.flatten(), prior_mean - prior_std, prior_mean + prior_std, color='tab:orange', alpha=0.3)
plt.show()

# %% [markdown]
# ## Computing the posterior
#
# The posterior distribution is proportional to the prior multiplied by a likelihood function. For this example we'll assume that the likelihood function is a Gaussian distribution. Using this, we can easily compute the posterior using the `*` operator.

# %%
likelihood = gpx.Gaussian(num_datapoints=D.n)
posterior = prior * likelihood

# %% [markdown]
# ### Stating parameters
#
# To store our model's hyperparameters, we suggest using a dictionary. In this example, our kernel is parameterised by a lengthscale and variance parameter. Further, our likelihood functions controls the amount of observation noise; the final parameter that we must optimise. These three quantities should therefore be defined as

# %%
params, training_status, constrainer, unconstrainer = gpx.initialise(posterior)
pp.pprint(params)

# %% [markdown]
# ### Parameter transformation
#
# The three parameters we defined earlier are all strictly positive. To ensure more stable optimisation, it is strongly advised to transform the parameters onto an unconstrained space first. Through the `transform` function and a given bijective transformation function this can be achieved as follows

# %%
params = gpx.transform(params, unconstrainer)

# %% [markdown]
# ## Optimisation
#
# To learn the optimal set of hyperparameters, we'll optimise the marginal log-likelihood of the posterior with respect to the hyperparameters. To realise this function, one can run the following.

# %%
from jax import jit

mll = jit(posterior.marginal_log_likelihood(D, constrainer, negative=True))
mll(params)

# %% [markdown]
# Note that most optimisers (including here) minimise a given function. Truly, we wish to maximise the marginal log-likelihood, so we instead realise the negative marginal log-likelihood so that the minimisation is equivalent to maximising the marginal log-likelihood.

# %% [markdown]
# ### Defining an optimiser
#
# We can now define an optimiser using one of the optimiser's supplied in Jax's `experimental` module. For this example we'll use the adam optimiser with a step-size of $0.01$.

# %%
from gpjax.abstractions import fit

opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)
final_params = fit(
    mll,
    params,
    training_status,
    opt_init,
    opt_update,
    get_params,
    n_iters=500,
    jit_compile=True,
)

# %% [markdown]
# ## Learned parameters
#
# The exact value of our learned parameters can often be useful in answering certain questions about the underlying process. To find these values, we should untransfom them so that they are redefined on the original constrained space.

# %%
final_params = gpx.transform(final_params, constrainer)
pp.pprint(final_params)

# %% [markdown]
# ## Prediction
#
# ### Direct prediction
#
# Equipped with a posterior distribution and a set of optimised hyperparameter values defined on their original parameter space, we are now in a position to query our GP's predictive posterior distribution at a set of test points. To do this, we can either compute the process' expectation and variance directly using the following `mean` and `variance` functions.

# %%
latent_dist = posterior(D, final_params)(xtest)
predictive_dist = likelihood(latent_dist, final_params)

predictive_mean = predictive_dist.mean()
predictive_std = predictive_dist.stddev()

# %% [markdown]
# With the predictive mean and variance acquired, we can now visualise how well our GP does at explaining the original data.

# %%
fig, ax = plt.subplots(figsize=(12, 5))
ax.plot(x, y, "o", label="Observations", color="tab:red")
ax.plot(xtest, predictive_mean, label="Predictive mean", color="tab:blue")
ax.fill_between(
    xtest.squeeze(),
    predictive_mean - predictive_std,
    predictive_mean + predictive_std,
    alpha=0.2,
    color="tab:blue",
    label='Two sigma',
)
ax.plot(xtest, predictive_mean - predictive_std, color="tab:blue", linestyle="--", linewidth=1)
ax.plot(xtest, predictive_mean + predictive_std, color="tab:blue", linestyle="--", linewidth=1)

ax.legend()

# %% [markdown]
# ## System information

# %%
# %reload_ext watermark
# %watermark -n -u -v -iv -w -a 'Thomas Pinder'
