{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6abeff",
   "metadata": {},
   "source": [
    "# Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7797d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from jax.experimental import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpjax as gpx\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cccb58",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We'll simulate a binary dataset where our inputs $x$ are sampled according to $x_i \\sim \\mathcal{U}(-1., 1.)$ for $1 \\leq i \\leq 100$. Our corresponding outputs will be calculated according to\n",
    "$$ y_i = 0.5*\\operatorname{sign}(\\cos(2*x + \\epsilon_i) + 0.5, $$\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0, 0.01)$. Note, the multiplication and addition of 0.5 is simply to ensure that our outputs are in $\\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.sort(jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0), axis=0)\n",
    "y = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(key, shape=x.shape) * 0.05)) + 0.5\n",
    "xtest = jnp.linspace(-1.05, 1.05, 500).reshape(-1, 1)\n",
    "plt.plot(x, y, \"o\", markersize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = gpx.Dataset(X=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a907a",
   "metadata": {},
   "source": [
    "We can now define our prior Gaussian process such that an RBF kernel has been selected for the purpose of exposition. However, an alternative kernel may be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = gpx.RBF()\n",
    "prior = gpx.Prior(kernel=kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d1627",
   "metadata": {},
   "source": [
    "Now we can proceed to define our likelihood function. In this example, our observations are binary, so we will select a Bernoulli likelihood. Using this likelihood function, we can compute the posterior through the product of our likelihood and prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior * gpx.Bernoulli(num_datapoints=training.n)\n",
    "print(type(posterior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a744335",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, constrainer, unconstrainer = gpx.initialise(posterior)\n",
    "params = gpx.transform(params, unconstrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ff7f3",
   "metadata": {},
   "source": [
    "With a posterior in place, we can estimate the maximum a posteriori using ObJax's optimisers. However, our Gaussian process is no longer conjugate, meaning that in addition to the kernel's hyperparameters, we are also tasked with learning the values of process' latent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = jax.jit(posterior.marginal_log_likelihood(training, constrainer, negative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpjax.abstractions import fit\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)\n",
    "optimised_params = fit(mll, params, opt_init, opt_update, get_params, n_iters=1000)\n",
    "optimised_params = gpx.transform(optimised_params, constrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = posterior.mean(training, optimised_params)(xtest)\n",
    "sigma = jnp.sqrt(posterior.variance(training, optimised_params)(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a7371",
   "metadata": {},
   "source": [
    "With the first and centralised second moment computed, we can plot these with the original data overlayed to confirm that our process has done a good job of recovering the latent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84767084",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x, y, \"o\", label=\"Obs\", color=\"tab:red\")\n",
    "ax.plot(xtest, mu, label=\"pred\", color=\"tab:blue\")\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    mu.squeeze() - sigma,\n",
    "    mu.squeeze() + sigma,\n",
    "    alpha=0.2,\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "ax.plot(xtest, mu.squeeze() - sigma, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "ax.plot(xtest, mu.squeeze() + sigma, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833435cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -a \"Thomas Pinder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPJax",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
